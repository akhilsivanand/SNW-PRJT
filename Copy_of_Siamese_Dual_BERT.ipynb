{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Copy of Siamese_Dual_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a075c7ae0083428087a66848e00fd99d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5bb06e3b38104664912ba0fc8d26c9b5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6e493fffb9314776ae970cca1498e8df",
              "IPY_MODEL_b0dbb1a30cb0439cb96a2b5364842472"
            ]
          }
        },
        "5bb06e3b38104664912ba0fc8d26c9b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6e493fffb9314776ae970cca1498e8df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4f95af9bc6634c1b8fc2f583f2b11667",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6c285883d4bc442381040afd90208791"
          }
        },
        "b0dbb1a30cb0439cb96a2b5364842472": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_769616b93f65452c8719bce846620880",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:01&lt;00:00, 406B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e11a31b293cf4c4881503f90007bd33d"
          }
        },
        "4f95af9bc6634c1b8fc2f583f2b11667": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6c285883d4bc442381040afd90208791": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "769616b93f65452c8719bce846620880": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e11a31b293cf4c4881503f90007bd33d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "595862c55a5e4ca3bc55d42c5c527695": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d3e0cfa830ac432697cfa69c99a30e1f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b7aab9e3907146cab494ef53ec9e666e",
              "IPY_MODEL_897bb7873ec64821baeb2af34ee79d73"
            ]
          }
        },
        "d3e0cfa830ac432697cfa69c99a30e1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b7aab9e3907146cab494ef53ec9e666e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_adc63c1e7cae48df801528911fa5b95b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cdc3f2e558e64fd9b45960e7da303a83"
          }
        },
        "897bb7873ec64821baeb2af34ee79d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5ac87b01bb4d4616920532eeb328cb6c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 742kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8cb3710f534f4fdbb718d33fc2d7107f"
          }
        },
        "adc63c1e7cae48df801528911fa5b95b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cdc3f2e558e64fd9b45960e7da303a83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5ac87b01bb4d4616920532eeb328cb6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8cb3710f534f4fdbb718d33fc2d7107f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "885aff8871374c7a8242c753c54f2e6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e9f51be120e74a198f3d3864cfbcda49",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_85847000dc464a29b91a4bca8d6974b2",
              "IPY_MODEL_9708c3019690413a9e5a548c4d324e91"
            ]
          }
        },
        "e9f51be120e74a198f3d3864cfbcda49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "85847000dc464a29b91a4bca8d6974b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4d487251c29f42fb94f4fc9f99249e9a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 687,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 687,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0e75905e9c154d5580d071ff064e9753"
          }
        },
        "9708c3019690413a9e5a548c4d324e91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_be27906e804f440094fc7fbb35fe8891",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 687/687 [00:02&lt;00:00, 233.46it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2bff995d92574cb1bc747e675943dff8"
          }
        },
        "4d487251c29f42fb94f4fc9f99249e9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0e75905e9c154d5580d071ff064e9753": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "be27906e804f440094fc7fbb35fe8891": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2bff995d92574cb1bc747e675943dff8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bbcf28031749431e88704146f1190c32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2037bab5544f4c76b19d6b411a7806fa",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_72a26b4a16a141519c30fc37b69c9370",
              "IPY_MODEL_9edba3f7a7ca459ebe6a343a30cdb05b"
            ]
          }
        },
        "2037bab5544f4c76b19d6b411a7806fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72a26b4a16a141519c30fc37b69c9370": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1dcfe96024fa4e6d94e56ee84611e61b",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 295,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 295,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15d6f36a97324d2d839faa1ee487a3b1"
          }
        },
        "9edba3f7a7ca459ebe6a343a30cdb05b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7850683e73c642a9a858747c2d493539",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 295/295 [00:28&lt;00:00, 10.21it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2a150d7b04564babad732e76f391ce23"
          }
        },
        "1dcfe96024fa4e6d94e56ee84611e61b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15d6f36a97324d2d839faa1ee487a3b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7850683e73c642a9a858747c2d493539": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2a150d7b04564babad732e76f391ce23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "351b2e2227f94b1485e9d479adda936c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_59bf63b862194808989c80760ba9fe6b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ae89fedcebb54d579e5e880bc84a7cf4",
              "IPY_MODEL_ce196d9edb6f4cc7b4db41eb6a1f1190"
            ]
          }
        },
        "59bf63b862194808989c80760ba9fe6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ae89fedcebb54d579e5e880bc84a7cf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_48511e18f1654c329757c10a31553c09",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 536063208,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 536063208,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ae0ceff12958463194af96dcd2c1f8f5"
          }
        },
        "ce196d9edb6f4cc7b4db41eb6a1f1190": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2a5e8a06d5044927ac7752ae18201f98",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 536M/536M [00:11&lt;00:00, 45.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8541cf96f7ab49409d1447480146d23d"
          }
        },
        "48511e18f1654c329757c10a31553c09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ae0ceff12958463194af96dcd2c1f8f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a5e8a06d5044927ac7752ae18201f98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8541cf96f7ab49409d1447480146d23d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhilsivanand/SNW-PRJT/blob/master/Copy_of_Siamese_Dual_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "nKUC6NaerKWb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "3a5e349e-6bfb-4ce3-9d28-ab398d5d634d"
      },
      "source": [
        "!pip install transformers\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\r\u001b[K     |▍                               | 10kB 19.5MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 4.9MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 6.2MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 6.4MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 5.4MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 5.9MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 6.3MB/s eta 0:00:01\r\u001b[K     |███▍                            | 81kB 6.9MB/s eta 0:00:01\r\u001b[K     |███▉                            | 92kB 6.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 102kB 6.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 133kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 143kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 153kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 163kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 174kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 184kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 194kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 204kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 215kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 225kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 235kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 245kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 256kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 266kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 276kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 286kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 296kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 307kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 317kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 327kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 337kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 348kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 358kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 368kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 378kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 389kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 399kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 409kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 419kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 430kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 440kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 450kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 460kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 471kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 481kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 491kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 501kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 512kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 522kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 532kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 542kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 552kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 563kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 573kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 583kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 593kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 604kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 614kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 624kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 634kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 645kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 655kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 665kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 675kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 686kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 696kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 706kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 716kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 727kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 737kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 747kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 757kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 768kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 778kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 19.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 41.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 39.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=222f98365247b5abb3c10ba5a8425e4bcef1036b7a398bf3a974ce21c93969ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPZYN5FdrpO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import itertools\n",
        "import random\n",
        "import os\n",
        "\n",
        "from transformers import TFAutoModel, AutoTokenizer, BertConfig, TFBertModel\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zvUxp4BrKWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title, fontsize=25)\n",
        "    #plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90, fontsize=15)\n",
        "    plt.yticks(tick_marks, classes, fontsize=15)\n",
        "\n",
        "    fmt = '.2f'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\", fontsize = 14)\n",
        "\n",
        "    plt.ylabel('True label', fontsize=20)\n",
        "    plt.xlabel('Predicted label', fontsize=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHkIzEcBs7jN",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "11580777-147f-427c-bc47-63fb2f9262dc"
      },
      "source": [
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-16652364-de67-41df-9021-494a09167622\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-16652364-de67-41df-9021-494a09167622\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving USER.csv to USER.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiF2JmB6s7v0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "dfTickets = pd.read_csv(io.BytesIO(uploaded['USER.csv']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCWRTMWt04YE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "outputId": "6f2c7a36-fd20-4785-c644-01d2f7ed5cb7"
      },
      "source": [
        "df = dfTickets\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Number</th>\n",
              "      <th>Short description</th>\n",
              "      <th>Assignment group</th>\n",
              "      <th>Close notes</th>\n",
              "      <th>Tags</th>\n",
              "      <th>Classification</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>INC3860455</td>\n",
              "      <td>SAS Permission issue in BI LAYER Production  a...</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>Issue Description: SAS Permission issue in BI ...</td>\n",
              "      <td>AZ Technology</td>\n",
              "      <td>SAS GPFS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>INC3800702</td>\n",
              "      <td>SAS EG error | workspace server failure</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>IT: Access Issue.\\r\\nD: SAS EG error | workspa...</td>\n",
              "      <td>AZ Reinsurance</td>\n",
              "      <td>SAS Platform Access/Availability</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>INC4789792</td>\n",
              "      <td>Extend Technical Users</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>Issue Description:Extend Technical Users\\r\\nTy...</td>\n",
              "      <td>AZ Technology</td>\n",
              "      <td>SAS Access Issue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>INC3308102</td>\n",
              "      <td>Request for information regarding jobs</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>REQ0807777</td>\n",
              "      <td>AZ Technology</td>\n",
              "      <td>SAS Service Requests</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>INC4216556</td>\n",
              "      <td>Datenrestore SAS BI (PaaS) Deutschland</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>Description :  Data restore\\r\\nType : Service ...</td>\n",
              "      <td>AZ Germany</td>\n",
              "      <td>SAS Service Requests</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1192</th>\n",
              "      <td>1217</td>\n",
              "      <td>INC4403047</td>\n",
              "      <td>Lost access to BIFI Tools for 3 users.</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>Incident Description: Lost access to BIFI Tool...</td>\n",
              "      <td>AZ France</td>\n",
              "      <td>SAS Access Issue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1193</th>\n",
              "      <td>1218</td>\n",
              "      <td>INC2642993</td>\n",
              "      <td>Unable to connect to TDBPCE project using SAS EG</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>The user was missing the required unix groups....</td>\n",
              "      <td>AZ Reinsurance</td>\n",
              "      <td>SAS Access Issue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1194</th>\n",
              "      <td>1219</td>\n",
              "      <td>INC3501151</td>\n",
              "      <td>SAS got frozen</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>Dear ZB,\\r\\nThanks for your mail confirmation ...</td>\n",
              "      <td>AZ Reinsurance</td>\n",
              "      <td>SAS Access Issue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1195</th>\n",
              "      <td>1220</td>\n",
              "      <td>INC3589899</td>\n",
              "      <td>Unable to connect to REDWH from SAS EG</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>We have guided user to submit a GIAM request t...</td>\n",
              "      <td>AZ Reinsurance</td>\n",
              "      <td>SAS Access Issue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1196</th>\n",
              "      <td>1221</td>\n",
              "      <td>INC2887515</td>\n",
              "      <td>SAS MIS10 Report issue</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>As the issue is with only one report (MIS 10) ...</td>\n",
              "      <td>AZ Reinsurance</td>\n",
              "      <td>SAS Others</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1197 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0      Number  ...            Tags                     Classification\n",
              "0              0  INC3860455  ...   AZ Technology                           SAS GPFS\n",
              "1              1  INC3800702  ...  AZ Reinsurance   SAS Platform Access/Availability\n",
              "2              2  INC4789792  ...   AZ Technology                   SAS Access Issue\n",
              "3              3  INC3308102  ...   AZ Technology               SAS Service Requests\n",
              "4              4  INC4216556  ...      AZ Germany               SAS Service Requests\n",
              "...          ...         ...  ...             ...                                ...\n",
              "1192        1217  INC4403047  ...       AZ France                   SAS Access Issue\n",
              "1193        1218  INC2642993  ...  AZ Reinsurance                   SAS Access Issue\n",
              "1194        1219  INC3501151  ...  AZ Reinsurance                   SAS Access Issue\n",
              "1195        1220  INC3589899  ...  AZ Reinsurance                   SAS Access Issue\n",
              "1196        1221  INC2887515  ...  AZ Reinsurance                         SAS Others\n",
              "\n",
              "[1197 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxfZN-gi2nEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def cleanDataset(dataset, columnsToClean, regexList):\n",
        "#     for column in columnsToClean:\n",
        "#         for regex in regexList:\n",
        "#             dataset[column] = removeString(dataset[column], regex)\n",
        "#     return dataset\n",
        "\n",
        "\n",
        "# def removeString(data, regex):\n",
        "#     return data.str.lower().str.replace(regex, ' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JesEWaQD2xQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def getRegexList():\n",
        "#     regexList = []\n",
        "\n",
        "#     regexList += ['^[_a-z0-9-]+(\\.[_a-z0-9-]+)*@[a-z0-9-]+(\\.[a-z0-9-]+)*(\\.[a-z]{2,4})$']\n",
        "#     regexList += ['[\\w\\d\\-\\_\\.]+ @ [\\w\\d\\-\\_\\.]+']\n",
        "#     regexList += ['[^a-zA-Z]']\n",
        "\n",
        "#     return regexList"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knejvF_h6Zu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "listOfStr = [' SAS Server Issue', ' SAS Platform Access/Availability',\n",
        "       ' SAS Access Issue', ' SAS Service Requests', ' SAS Backup', ' SAS GPFS',\n",
        "       ' SAS -   UC4  Job Issues', ' SAS - Space Issue',\n",
        "       ' SAS Grafana Alert', ' SAS AVC Client Issue',\n",
        "       ' SAS Database', ' SAS Performance',\n",
        "       ' SAS AVC Client Issue - Vitualization',\n",
        "       ' SAS Portal issue',\n",
        "       ' sas Backup']\n",
        "map_label = { i : listOfStr[i] for i in range(0, len(listOfStr) ) }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZWb1u1T6cPT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "33f2630c-0c51-4a09-d5a5-659d44e4fc5c"
      },
      "source": [
        "map_label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: ' SAS Server Issue',\n",
              " 1: ' SAS Platform Access/Availability',\n",
              " 2: ' SAS Access Issue',\n",
              " 3: ' SAS Service Requests',\n",
              " 4: ' SAS Backup',\n",
              " 5: ' SAS GPFS',\n",
              " 6: ' SAS -   UC4  Job Issues',\n",
              " 7: ' SAS - Space Issue',\n",
              " 8: ' SAS Grafana Alert',\n",
              " 9: ' SAS AVC Client Issue',\n",
              " 10: ' SAS Database',\n",
              " 11: ' SAS Performance',\n",
              " 12: ' SAS AVC Client Issue - Vitualization',\n",
              " 13: ' SAS Portal issue',\n",
              " 14: ' sas Backup'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKD9wkCj6irG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.loc[df['Classification'].isin(listOfStr)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtVU9UBm6vaW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "outputId": "6c6a799a-cf77-4a6e-d218-6e47a304ec42"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Number</th>\n",
              "      <th>Short description</th>\n",
              "      <th>Assignment group</th>\n",
              "      <th>Close notes</th>\n",
              "      <th>Tags</th>\n",
              "      <th>Classification</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>INC3860455</td>\n",
              "      <td>SAS Permission issue in BI LAYER Production  a...</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>Issue Description: SAS Permission issue in BI ...</td>\n",
              "      <td>AZ Technology</td>\n",
              "      <td>SAS GPFS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>INC3800702</td>\n",
              "      <td>SAS EG error | workspace server failure</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>IT: Access Issue.\\r\\nD: SAS EG error | workspa...</td>\n",
              "      <td>AZ Reinsurance</td>\n",
              "      <td>SAS Platform Access/Availability</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>INC4789792</td>\n",
              "      <td>Extend Technical Users</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>Issue Description:Extend Technical Users\\r\\nTy...</td>\n",
              "      <td>AZ Technology</td>\n",
              "      <td>SAS Access Issue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>INC3308102</td>\n",
              "      <td>Request for information regarding jobs</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>REQ0807777</td>\n",
              "      <td>AZ Technology</td>\n",
              "      <td>SAS Service Requests</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>INC4216556</td>\n",
              "      <td>Datenrestore SAS BI (PaaS) Deutschland</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>Description :  Data restore\\r\\nType : Service ...</td>\n",
              "      <td>AZ Germany</td>\n",
              "      <td>SAS Service Requests</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1191</th>\n",
              "      <td>1216</td>\n",
              "      <td>INC2984010</td>\n",
              "      <td>PROD COP (SLA13002)</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>The request was completed</td>\n",
              "      <td>AZ France</td>\n",
              "      <td>SAS Server Issue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1192</th>\n",
              "      <td>1217</td>\n",
              "      <td>INC4403047</td>\n",
              "      <td>Lost access to BIFI Tools for 3 users.</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>Incident Description: Lost access to BIFI Tool...</td>\n",
              "      <td>AZ France</td>\n",
              "      <td>SAS Access Issue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1193</th>\n",
              "      <td>1218</td>\n",
              "      <td>INC2642993</td>\n",
              "      <td>Unable to connect to TDBPCE project using SAS EG</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>The user was missing the required unix groups....</td>\n",
              "      <td>AZ Reinsurance</td>\n",
              "      <td>SAS Access Issue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1194</th>\n",
              "      <td>1219</td>\n",
              "      <td>INC3501151</td>\n",
              "      <td>SAS got frozen</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>Dear ZB,\\r\\nThanks for your mail confirmation ...</td>\n",
              "      <td>AZ Reinsurance</td>\n",
              "      <td>SAS Access Issue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1195</th>\n",
              "      <td>1220</td>\n",
              "      <td>INC3589899</td>\n",
              "      <td>Unable to connect to REDWH from SAS EG</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>We have guided user to submit a GIAM request t...</td>\n",
              "      <td>AZ Reinsurance</td>\n",
              "      <td>SAS Access Issue</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>982 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0      Number  ...            Tags                     Classification\n",
              "0              0  INC3860455  ...   AZ Technology                           SAS GPFS\n",
              "1              1  INC3800702  ...  AZ Reinsurance   SAS Platform Access/Availability\n",
              "2              2  INC4789792  ...   AZ Technology                   SAS Access Issue\n",
              "3              3  INC3308102  ...   AZ Technology               SAS Service Requests\n",
              "4              4  INC4216556  ...      AZ Germany               SAS Service Requests\n",
              "...          ...         ...  ...             ...                                ...\n",
              "1191        1216  INC2984010  ...       AZ France                   SAS Server Issue\n",
              "1192        1217  INC4403047  ...       AZ France                   SAS Access Issue\n",
              "1193        1218  INC2642993  ...  AZ Reinsurance                   SAS Access Issue\n",
              "1194        1219  INC3501151  ...  AZ Reinsurance                   SAS Access Issue\n",
              "1195        1220  INC3589899  ...  AZ Reinsurance                   SAS Access Issue\n",
              "\n",
              "[982 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJXG0rp028E_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# columnsToClean = ['Short description']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KIF5uw0209N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df =cleanDataset(df, columnsToClean, getRegexList())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvGrzkQ3s70H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "e7d46aa9-9982-4aec-9fa8-cd98a6ef6c11"
      },
      "source": [
        "df['Classification'] = df['Classification'].factorize()[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URFgVxOFOq6Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "a0a82968-b51f-4f4a-bfb7-877d9751e165"
      },
      "source": [
        "df['Classification'] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       0\n",
              "1       1\n",
              "2       2\n",
              "3       3\n",
              "4       3\n",
              "       ..\n",
              "1191    6\n",
              "1192    2\n",
              "1193    2\n",
              "1194    2\n",
              "1195    2\n",
              "Name: Classification, Length: 982, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-2-b7_FMYuc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "outputId": "8347683b-d66e-4f37-878f-3a828cc98b5f"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Number</th>\n",
              "      <th>Short description</th>\n",
              "      <th>Assignment group</th>\n",
              "      <th>Close notes</th>\n",
              "      <th>Tags</th>\n",
              "      <th>Classification</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>INC3860455</td>\n",
              "      <td>SAS Permission issue in BI LAYER Production  a...</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>Issue Description: SAS Permission issue in BI ...</td>\n",
              "      <td>AZ Technology</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>INC3800702</td>\n",
              "      <td>SAS EG error | workspace server failure</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>IT: Access Issue.\\r\\nD: SAS EG error | workspa...</td>\n",
              "      <td>AZ Reinsurance</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>INC4789792</td>\n",
              "      <td>Extend Technical Users</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>Issue Description:Extend Technical Users\\r\\nTy...</td>\n",
              "      <td>AZ Technology</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>INC3308102</td>\n",
              "      <td>Request for information regarding jobs</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>REQ0807777</td>\n",
              "      <td>AZ Technology</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>INC4216556</td>\n",
              "      <td>Datenrestore SAS BI (PaaS) Deutschland</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>Description :  Data restore\\r\\nType : Service ...</td>\n",
              "      <td>AZ Germany</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1191</th>\n",
              "      <td>1216</td>\n",
              "      <td>INC2984010</td>\n",
              "      <td>PROD COP (SLA13002)</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>The request was completed</td>\n",
              "      <td>AZ France</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1192</th>\n",
              "      <td>1217</td>\n",
              "      <td>INC4403047</td>\n",
              "      <td>Lost access to BIFI Tools for 3 users.</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>Incident Description: Lost access to BIFI Tool...</td>\n",
              "      <td>AZ France</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1193</th>\n",
              "      <td>1218</td>\n",
              "      <td>INC2642993</td>\n",
              "      <td>Unable to connect to TDBPCE project using SAS EG</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>The user was missing the required unix groups....</td>\n",
              "      <td>AZ Reinsurance</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1194</th>\n",
              "      <td>1219</td>\n",
              "      <td>INC3501151</td>\n",
              "      <td>SAS got frozen</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>Dear ZB,\\r\\nThanks for your mail confirmation ...</td>\n",
              "      <td>AZ Reinsurance</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1195</th>\n",
              "      <td>1220</td>\n",
              "      <td>INC3589899</td>\n",
              "      <td>Unable to connect to REDWH from SAS EG</td>\n",
              "      <td>AZTEC.GAP.PAAS-SAS-CL-2ND</td>\n",
              "      <td>We have guided user to submit a GIAM request t...</td>\n",
              "      <td>AZ Reinsurance</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>982 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0      Number  ...            Tags Classification\n",
              "0              0  INC3860455  ...   AZ Technology              0\n",
              "1              1  INC3800702  ...  AZ Reinsurance              1\n",
              "2              2  INC4789792  ...   AZ Technology              2\n",
              "3              3  INC3308102  ...   AZ Technology              3\n",
              "4              4  INC4216556  ...      AZ Germany              3\n",
              "...          ...         ...  ...             ...            ...\n",
              "1191        1216  INC2984010  ...       AZ France              6\n",
              "1192        1217  INC4403047  ...       AZ France              2\n",
              "1193        1218  INC2642993  ...  AZ Reinsurance              2\n",
              "1194        1219  INC3501151  ...  AZ Reinsurance              2\n",
              "1195        1220  INC3589899  ...  AZ Reinsurance              2\n",
              "\n",
              "[982 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "H-GcH7vVrKWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXMqsQu_6QMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr1dcmuhrKWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### UTILITY FUNCTIONS FOR TOKENIZATIONS, MASKS AND SEGMENTS CREATION ###\n",
        "### from: https://www.kaggle.com/akensert/bert-base-tf2-0-now-huggingface-transformer\n",
        "\n",
        "def convert_to_transformer_inputs(str1, str2, tokenizer, max_sequence_length, double=True):\n",
        "    \n",
        "    def return_id(str1, str2, truncation_strategy, length):\n",
        "\n",
        "        inputs = tokenizer.encode_plus(str1, str2,\n",
        "            add_special_tokens=True,\n",
        "            max_length=length,\n",
        "            truncation_strategy=truncation_strategy)\n",
        "        \n",
        "        input_ids =  inputs[\"input_ids\"]\n",
        "        input_masks = [1] * len(input_ids)\n",
        "        input_segments = inputs[\"token_type_ids\"]\n",
        "        \n",
        "        padding_length = length - len(input_ids)\n",
        "        padding_id = tokenizer.pad_token_id\n",
        "        \n",
        "        input_ids = input_ids + ([padding_id] * padding_length)\n",
        "        input_masks = input_masks + ([0] * padding_length)\n",
        "        input_segments = input_segments + ([0] * padding_length)\n",
        "        \n",
        "        return [input_ids, input_masks, input_segments]\n",
        "    \n",
        "    if double:\n",
        "    \n",
        "        input_ids_1, input_masks_1, input_segments_1 = return_id(\n",
        "            str1, None, 'longest_first', max_sequence_length)\n",
        "\n",
        "        input_ids_2, input_masks_2, input_segments_2 = return_id(\n",
        "            str2, None, 'longest_first', max_sequence_length)\n",
        "\n",
        "        return [input_ids_1, input_masks_1, input_segments_1,\n",
        "                input_ids_2, input_masks_2, input_segments_2]\n",
        "    \n",
        "    else:\n",
        "        \n",
        "        input_ids, input_masks, input_segments = return_id(\n",
        "            str1, str2, 'longest_first', max_sequence_length)\n",
        "\n",
        "        return [input_ids, input_masks, input_segments,\n",
        "                None, None, None]        \n",
        "\n",
        "def compute_input_arrays(df, columns, tokenizer, max_sequence_length, double=True):\n",
        "    \n",
        "    input_ids_1, input_masks_1, input_segments_1 = [], [], []\n",
        "    input_ids_2, input_masks_2, input_segments_2 = [], [], []\n",
        "    for _, instance in tqdm(df[columns].iterrows(), total=len(df)):\n",
        "        str1, str2 = instance[columns[0]], instance[columns[1]]\n",
        "\n",
        "        ids_1, masks_1, segments_1, ids_2, masks_2, segments_2 = \\\n",
        "        convert_to_transformer_inputs(str1, str2, tokenizer, max_sequence_length, double=double)\n",
        "        \n",
        "        input_ids_1.append(ids_1)\n",
        "        input_masks_1.append(masks_1)\n",
        "        input_segments_1.append(segments_1)\n",
        "\n",
        "        input_ids_2.append(ids_2)\n",
        "        input_masks_2.append(masks_2)\n",
        "        input_segments_2.append(segments_2)\n",
        "        \n",
        "    if double:\n",
        "        \n",
        "        return [np.asarray(input_ids_1, dtype=np.int32), \n",
        "                np.asarray(input_masks_1, dtype=np.int32), \n",
        "                np.asarray(input_segments_1, dtype=np.int32),\n",
        "                np.asarray(input_ids_2, dtype=np.int32), \n",
        "                np.asarray(input_masks_2, dtype=np.int32), \n",
        "                np.asarray(input_segments_2, dtype=np.int32)]\n",
        "    \n",
        "    else:\n",
        "        \n",
        "        return [np.asarray(input_ids_1, dtype=np.int32), \n",
        "                np.asarray(input_masks_1, dtype=np.int32), \n",
        "                np.asarray(input_segments_1, dtype=np.int32)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg-4tTSQrKWr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f0848773-1e54-4fd1-a82e-f30b8d83c2cb"
      },
      "source": [
        "### TRAIN TEST SPLIT ###\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[['Number','Short description']], df['Classification'].values, \n",
        "                                                    random_state=33, test_size = 0.3)\n",
        "\n",
        "\n",
        "print(X_train.shape, X_test.shape)\n",
        "print(y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(687, 2) (295, 2)\n",
            "(687,) (295,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZlmhNJU4ty3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "3607101f-4309-4b9f-a870-c05bfee024f0"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Number</th>\n",
              "      <th>Short description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>369</th>\n",
              "      <td>INC2742369</td>\n",
              "      <td>SAS - Missing Permission</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>INC2541642</td>\n",
              "      <td>Issu with programm running</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380</th>\n",
              "      <td>INC4356813</td>\n",
              "      <td>Can You please add me and my colleagues (Laszl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>INC4246677</td>\n",
              "      <td>'SAS DEV environment stopped working. Now is w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>756</th>\n",
              "      <td>INC2478821</td>\n",
              "      <td>Win7 - SAS Enterprise Guide - Server nicht ver...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>794</th>\n",
              "      <td>INC2840055</td>\n",
              "      <td>SAS: Copy AUTHDOMAIN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>701</th>\n",
              "      <td>INC4124580</td>\n",
              "      <td>Excel file can not be opened in SAS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>873</th>\n",
              "      <td>INC4375543</td>\n",
              "      <td>(R) Incompatibility problems of SAS AVC package</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481</th>\n",
              "      <td>INC3008291</td>\n",
              "      <td>SAS - Application Virtualization Fehler - Anwe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>INC3774678</td>\n",
              "      <td>Query on Scheduled Jobs</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>687 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Number                                  Short description\n",
              "369  INC2742369                           SAS - Missing Permission\n",
              "269  INC2541642                         Issu with programm running\n",
              "380  INC4356813  Can You please add me and my colleagues (Laszl...\n",
              "283  INC4246677  'SAS DEV environment stopped working. Now is w...\n",
              "756  INC2478821  Win7 - SAS Enterprise Guide - Server nicht ver...\n",
              "..          ...                                                ...\n",
              "794  INC2840055                               SAS: Copy AUTHDOMAIN\n",
              "701  INC4124580                Excel file can not be opened in SAS\n",
              "873  INC4375543    (R) Incompatibility problems of SAS AVC package\n",
              "481  INC3008291  SAS - Application Virtualization Fehler - Anwe...\n",
              "31   INC3774678                            Query on Scheduled Jobs\n",
              "\n",
              "[687 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKNdt02KrKWv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "a075c7ae0083428087a66848e00fd99d",
            "5bb06e3b38104664912ba0fc8d26c9b5",
            "6e493fffb9314776ae970cca1498e8df",
            "b0dbb1a30cb0439cb96a2b5364842472",
            "4f95af9bc6634c1b8fc2f583f2b11667",
            "6c285883d4bc442381040afd90208791",
            "769616b93f65452c8719bce846620880",
            "e11a31b293cf4c4881503f90007bd33d",
            "595862c55a5e4ca3bc55d42c5c527695",
            "d3e0cfa830ac432697cfa69c99a30e1f",
            "b7aab9e3907146cab494ef53ec9e666e",
            "897bb7873ec64821baeb2af34ee79d73",
            "adc63c1e7cae48df801528911fa5b95b",
            "cdc3f2e558e64fd9b45960e7da303a83",
            "5ac87b01bb4d4616920532eeb328cb6c",
            "8cb3710f534f4fdbb718d33fc2d7107f"
          ]
        },
        "outputId": "6edbc950-8bce-4c06-9f46-9f438c4f6a80"
      },
      "source": [
        "\n",
        "### IMPORT TOKENIZER ###\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 300\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a075c7ae0083428087a66848e00fd99d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "595862c55a5e4ca3bc55d42c5c527695",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YlNhydJrKWy",
        "colab_type": "text"
      },
      "source": [
        "# **SIMPLE BERT (ONE INPUT)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA7GYrjbrKWy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "885aff8871374c7a8242c753c54f2e6e",
            "e9f51be120e74a198f3d3864cfbcda49",
            "85847000dc464a29b91a4bca8d6974b2",
            "9708c3019690413a9e5a548c4d324e91",
            "4d487251c29f42fb94f4fc9f99249e9a",
            "0e75905e9c154d5580d071ff064e9753",
            "be27906e804f440094fc7fbb35fe8891",
            "2bff995d92574cb1bc747e675943dff8",
            "bbcf28031749431e88704146f1190c32",
            "2037bab5544f4c76b19d6b411a7806fa",
            "72a26b4a16a141519c30fc37b69c9370",
            "9edba3f7a7ca459ebe6a343a30cdb05b",
            "1dcfe96024fa4e6d94e56ee84611e61b",
            "15d6f36a97324d2d839faa1ee487a3b1",
            "7850683e73c642a9a858747c2d493539",
            "2a150d7b04564babad732e76f391ce23"
          ]
        },
        "outputId": "0fb08dc4-6f53-428b-a27a-864101d5045c"
      },
      "source": [
        "### CREATE SEQUENCES (id, mask, segments) FOR TRAIN AND TEST ###\n",
        "\n",
        "input_train = compute_input_arrays(X_train, ['Number','Short description'], tokenizer, MAX_SEQUENCE_LENGTH, double=False)\n",
        "input_test = compute_input_arrays(X_test, ['Number','Short description'], tokenizer, MAX_SEQUENCE_LENGTH, double=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "885aff8871374c7a8242c753c54f2e6e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=687.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbcf28031749431e88704146f1190c32",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSK1t55MrKW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simple_bert():\n",
        "    \n",
        "    opt = Adam(learning_rate=2e-5)\n",
        "    \n",
        "    id_ = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    mask_ = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    atn_ = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    \n",
        "    config = BertConfig()\n",
        "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
        "    bert_model = TFBertModel.from_pretrained('bert-base-uncased', config=config)\n",
        "    \n",
        "    embedding = bert_model(id_, attention_mask=mask_, token_type_ids=atn_)[0]\n",
        "    \n",
        "    x = GlobalAveragePooling1D()(embedding)    \n",
        "    x = Dropout(0.2)(x)\n",
        "    out = Dense(len(map_label), activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=[id_, mask_, atn_], outputs=out)\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=opt)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQHOO2bPrKW5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630,
          "referenced_widgets": [
            "351b2e2227f94b1485e9d479adda936c",
            "59bf63b862194808989c80760ba9fe6b",
            "ae89fedcebb54d579e5e880bc84a7cf4",
            "ce196d9edb6f4cc7b4db41eb6a1f1190",
            "48511e18f1654c329757c10a31553c09",
            "ae0ceff12958463194af96dcd2c1f8f5",
            "2a5e8a06d5044927ac7752ae18201f98",
            "8541cf96f7ab49409d1447480146d23d"
          ]
        },
        "outputId": "24a760b2-e8ee-4c5c-8f47-63628737b315"
      },
      "source": [
        "tf.random.set_seed(33)\n",
        "os.environ['PYTHONHASHSEED'] = str(33)\n",
        "np.random.seed(33)\n",
        "random.seed(33)\n",
        "\n",
        "session_conf = tf.compat.v1.ConfigProto(\n",
        "    intra_op_parallelism_threads=1, \n",
        "    inter_op_parallelism_threads=1\n",
        ")\n",
        "sess = tf.compat.v1.Session(\n",
        "    graph=tf.compat.v1.get_default_graph(), \n",
        "    config=session_conf\n",
        ")\n",
        "tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "model = simple_bert()\n",
        "model.fit(input_train, y_train, epochs=11, batch_size=6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "351b2e2227f94b1485e9d479adda936c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=536063208.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/11\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            "115/115 [==============================] - 90s 779ms/step - loss: 2.0467\n",
            "Epoch 2/11\n",
            "115/115 [==============================] - 89s 778ms/step - loss: 1.3284\n",
            "Epoch 3/11\n",
            "115/115 [==============================] - 89s 778ms/step - loss: 0.7970\n",
            "Epoch 4/11\n",
            "115/115 [==============================] - 89s 778ms/step - loss: 0.4475\n",
            "Epoch 5/11\n",
            "115/115 [==============================] - 90s 779ms/step - loss: 0.2769\n",
            "Epoch 6/11\n",
            "115/115 [==============================] - 89s 778ms/step - loss: 0.1460\n",
            "Epoch 7/11\n",
            "115/115 [==============================] - 89s 777ms/step - loss: 0.1275\n",
            "Epoch 8/11\n",
            "115/115 [==============================] - 89s 777ms/step - loss: 0.0807\n",
            "Epoch 9/11\n",
            "115/115 [==============================] - 89s 776ms/step - loss: 0.0889\n",
            "Epoch 10/11\n",
            "115/115 [==============================] - 89s 777ms/step - loss: 0.0678\n",
            "Epoch 11/11\n",
            "115/115 [==============================] - 89s 777ms/step - loss: 0.0554\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb6e4771dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-DVy-gorKW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### PREDICT TEST ###\n",
        "\n",
        "pred_test = np.argmax(model.predict(input_test), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DURKbLVvrKXA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "441fdd6d-5b6a-4ae9-b91c-d26d3cb6773e"
      },
      "source": [
        "print(classification_report([map_label[i] for i in y_test], [map_label[i] for i in pred_test]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                       precision    recall  f1-score   support\n",
            "\n",
            "              SAS -   UC4  Job Issues       0.40      0.40      0.40        47\n",
            "                    SAS - Space Issue       0.00      0.00      0.00         2\n",
            "                 SAS AVC Client Issue       0.54      0.62      0.58        21\n",
            " SAS AVC Client Issue - Vitualization       1.00      1.00      1.00         4\n",
            "                     SAS Access Issue       0.49      0.61      0.54        59\n",
            "                           SAS Backup       0.75      0.60      0.67         5\n",
            "                         SAS Database       0.93      1.00      0.96        13\n",
            "                             SAS GPFS       0.44      0.36      0.40        11\n",
            "                    SAS Grafana Alert       1.00      1.00      1.00         2\n",
            "                      SAS Performance       0.47      0.47      0.47        15\n",
            "     SAS Platform Access/Availability       0.24      0.28      0.26        25\n",
            "                     SAS Portal issue       0.80      0.33      0.47        12\n",
            "                     SAS Server Issue       0.25      0.14      0.18         7\n",
            "                 SAS Service Requests       0.84      0.74      0.79        72\n",
            "\n",
            "                             accuracy                           0.56       295\n",
            "                            macro avg       0.58      0.54      0.55       295\n",
            "                         weighted avg       0.58      0.56      0.56       295\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQkY-M-ZrKXD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        },
        "outputId": "1f296136-a1df-499b-c294-46beb4d73ed7"
      },
      "source": [
        "\n",
        "cnf_matrix = confusion_matrix([map_label[i] for i in y_test], \n",
        "                              [map_label[i] for i in pred_test])\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "plot_confusion_matrix(cnf_matrix, classes=list(map_label.values()))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpEzmqnQRJma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HL1Kb1wFe9S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f38efc28-6ed3-4728-d9eb-a7cb28c88992"
      },
      "source": [
        "pip freeze\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "absl-py==0.9.0\n",
            "alabaster==0.7.12\n",
            "albumentations==0.1.12\n",
            "altair==4.1.0\n",
            "argon2-cffi==20.1.0\n",
            "asgiref==3.2.10\n",
            "astor==0.8.1\n",
            "astropy==4.0.1.post1\n",
            "astunparse==1.6.3\n",
            "atari-py==0.2.6\n",
            "atomicwrites==1.4.0\n",
            "attrs==19.3.0\n",
            "audioread==2.1.8\n",
            "autograd==1.3\n",
            "Babel==2.8.0\n",
            "backcall==0.2.0\n",
            "beautifulsoup4==4.6.3\n",
            "bleach==3.1.5\n",
            "blis==0.4.1\n",
            "bokeh==2.1.1\n",
            "boto==2.49.0\n",
            "boto3==1.14.33\n",
            "botocore==1.17.33\n",
            "Bottleneck==1.3.2\n",
            "branca==0.4.1\n",
            "bs4==0.0.1\n",
            "CacheControl==0.12.6\n",
            "cachetools==4.1.1\n",
            "catalogue==1.0.0\n",
            "certifi==2020.6.20\n",
            "cffi==1.14.1\n",
            "chainer==7.4.0\n",
            "chardet==3.0.4\n",
            "click==7.1.2\n",
            "cloudpickle==1.3.0\n",
            "cmake==3.12.0\n",
            "cmdstanpy==0.4.0\n",
            "colorlover==0.3.0\n",
            "community==1.0.0b1\n",
            "contextlib2==0.5.5\n",
            "convertdate==2.2.1\n",
            "coverage==3.7.1\n",
            "coveralls==0.5\n",
            "crcmod==1.7\n",
            "cufflinks==0.17.3\n",
            "cupy-cuda101==7.4.0\n",
            "cvxopt==1.2.5\n",
            "cvxpy==1.0.31\n",
            "cycler==0.10.0\n",
            "cymem==2.0.3\n",
            "Cython==0.29.21\n",
            "daft==0.0.4\n",
            "dask==2.12.0\n",
            "dataclasses==0.7\n",
            "datascience==0.10.6\n",
            "decorator==4.4.2\n",
            "defusedxml==0.6.0\n",
            "descartes==1.1.0\n",
            "dill==0.3.2\n",
            "distributed==1.25.3\n",
            "Django==3.0.8\n",
            "dlib==19.18.0\n",
            "dm-tree==0.1.5\n",
            "docopt==0.6.2\n",
            "docutils==0.15.2\n",
            "dopamine-rl==1.0.5\n",
            "earthengine-api==0.1.229\n",
            "easydict==1.9\n",
            "ecos==2.0.7.post1\n",
            "editdistance==0.5.3\n",
            "en-core-web-sm==2.2.5\n",
            "entrypoints==0.3\n",
            "ephem==3.7.7.1\n",
            "et-xmlfile==1.0.1\n",
            "fa2==0.3.5\n",
            "fancyimpute==0.4.3\n",
            "fastai==1.0.61\n",
            "fastdtw==0.3.4\n",
            "fastprogress==0.2.4\n",
            "fastrlock==0.5\n",
            "fbprophet==0.6\n",
            "feather-format==0.4.1\n",
            "featuretools==0.4.1\n",
            "filelock==3.0.12\n",
            "firebase-admin==4.1.0\n",
            "fix-yahoo-finance==0.0.22\n",
            "Flask==1.1.2\n",
            "folium==0.8.3\n",
            "fsspec==0.8.0\n",
            "future==0.16.0\n",
            "gast==0.3.3\n",
            "GDAL==2.2.2\n",
            "gdown==3.6.4\n",
            "gensim==3.6.0\n",
            "geographiclib==1.50\n",
            "geopy==1.17.0\n",
            "gin-config==0.3.0\n",
            "glob2==0.7\n",
            "google==2.0.3\n",
            "google-api-core==1.16.0\n",
            "google-api-python-client==1.7.12\n",
            "google-auth==1.17.2\n",
            "google-auth-httplib2==0.0.4\n",
            "google-auth-oauthlib==0.4.1\n",
            "google-cloud-bigquery==1.21.0\n",
            "google-cloud-core==1.0.3\n",
            "google-cloud-datastore==1.8.0\n",
            "google-cloud-firestore==1.7.0\n",
            "google-cloud-language==1.2.0\n",
            "google-cloud-storage==1.18.1\n",
            "google-cloud-translate==1.5.0\n",
            "google-colab==1.0.0\n",
            "google-pasta==0.2.0\n",
            "google-resumable-media==0.4.1\n",
            "googleapis-common-protos==1.52.0\n",
            "googledrivedownloader==0.4\n",
            "graphviz==0.10.1\n",
            "grpcio==1.30.0\n",
            "gspread==3.0.1\n",
            "gspread-dataframe==3.0.7\n",
            "gym==0.17.2\n",
            "h5py==2.10.0\n",
            "HeapDict==1.0.1\n",
            "holidays==0.9.12\n",
            "holoviews==1.13.3\n",
            "html5lib==1.0.1\n",
            "httpimport==0.5.18\n",
            "httplib2==0.17.4\n",
            "httplib2shim==0.0.3\n",
            "humanize==0.5.1\n",
            "hyperopt==0.1.2\n",
            "ideep4py==2.0.0.post3\n",
            "idna==2.10\n",
            "image==1.5.32\n",
            "imageio==2.4.1\n",
            "imagesize==1.2.0\n",
            "imbalanced-learn==0.4.3\n",
            "imblearn==0.0\n",
            "imgaug==0.2.9\n",
            "importlib-metadata==1.7.0\n",
            "imutils==0.5.3\n",
            "inflect==2.1.0\n",
            "iniconfig==1.0.1\n",
            "intel-openmp==2020.0.133\n",
            "intervaltree==2.1.0\n",
            "ipykernel==4.10.1\n",
            "ipython==5.5.0\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.3.9\n",
            "ipywidgets==7.5.1\n",
            "itsdangerous==1.1.0\n",
            "jax==0.1.73\n",
            "jaxlib==0.1.52\n",
            "jdcal==1.4.1\n",
            "jedi==0.17.2\n",
            "jieba==0.42.1\n",
            "Jinja2==2.11.2\n",
            "jmespath==0.10.0\n",
            "joblib==0.16.0\n",
            "jpeg4py==0.1.4\n",
            "jsonschema==2.6.0\n",
            "jupyter==1.0.0\n",
            "jupyter-client==5.3.5\n",
            "jupyter-console==5.2.0\n",
            "jupyter-core==4.6.3\n",
            "kaggle==1.5.6\n",
            "kapre==0.1.3.1\n",
            "Keras==2.4.3\n",
            "Keras-Preprocessing==1.1.2\n",
            "keras-vis==0.4.1\n",
            "kiwisolver==1.2.0\n",
            "knnimpute==0.1.0\n",
            "librosa==0.6.3\n",
            "lightgbm==2.2.3\n",
            "llvmlite==0.31.0\n",
            "lmdb==0.98\n",
            "lucid==0.3.8\n",
            "LunarCalendar==0.0.9\n",
            "lxml==4.2.6\n",
            "Markdown==3.2.2\n",
            "MarkupSafe==1.1.1\n",
            "matplotlib==3.2.2\n",
            "matplotlib-venn==0.11.5\n",
            "missingno==0.4.2\n",
            "mistune==0.8.4\n",
            "mizani==0.6.0\n",
            "mkl==2019.0\n",
            "mlxtend==0.14.0\n",
            "more-itertools==8.4.0\n",
            "moviepy==0.2.3.5\n",
            "mpmath==1.1.0\n",
            "msgpack==1.0.0\n",
            "multiprocess==0.70.10\n",
            "multitasking==0.0.9\n",
            "murmurhash==1.0.2\n",
            "music21==5.5.0\n",
            "natsort==5.5.0\n",
            "nbconvert==5.6.1\n",
            "nbformat==5.0.7\n",
            "networkx==2.4\n",
            "nibabel==3.0.2\n",
            "nltk==3.2.5\n",
            "notebook==5.3.1\n",
            "np-utils==0.5.12.1\n",
            "numba==0.48.0\n",
            "numexpr==2.7.1\n",
            "numpy==1.18.5\n",
            "nvidia-ml-py3==7.352.0\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.1.0\n",
            "okgrade==0.4.3\n",
            "opencv-contrib-python==4.1.2.30\n",
            "opencv-python==4.1.2.30\n",
            "openpyxl==2.5.9\n",
            "opt-einsum==3.3.0\n",
            "osqp==0.6.1\n",
            "packaging==20.4\n",
            "palettable==3.3.0\n",
            "pandas==1.0.5\n",
            "pandas-datareader==0.8.1\n",
            "pandas-gbq==0.11.0\n",
            "pandas-profiling==1.4.1\n",
            "pandocfilters==1.4.2\n",
            "panel==0.9.7\n",
            "param==1.9.3\n",
            "parso==0.7.1\n",
            "pathlib==1.0.1\n",
            "patsy==0.5.1\n",
            "pexpect==4.8.0\n",
            "pickleshare==0.7.5\n",
            "Pillow==7.0.0\n",
            "pip-tools==4.5.1\n",
            "plac==1.1.3\n",
            "plotly==4.4.1\n",
            "plotnine==0.6.0\n",
            "pluggy==0.7.1\n",
            "portpicker==1.3.1\n",
            "prefetch-generator==1.0.1\n",
            "preshed==3.0.2\n",
            "prettytable==0.7.2\n",
            "progressbar2==3.38.0\n",
            "prometheus-client==0.8.0\n",
            "promise==2.3\n",
            "prompt-toolkit==1.0.18\n",
            "protobuf==3.12.4\n",
            "psutil==5.4.8\n",
            "psycopg2==2.7.6.1\n",
            "ptyprocess==0.6.0\n",
            "py==1.9.0\n",
            "pyarrow==0.14.1\n",
            "pyasn1==0.4.8\n",
            "pyasn1-modules==0.2.8\n",
            "pycocotools==2.0.1\n",
            "pycparser==2.20\n",
            "pyct==0.4.6\n",
            "pydata-google-auth==1.1.0\n",
            "pydot==1.3.0\n",
            "pydot-ng==2.0.0\n",
            "pydotplus==2.0.2\n",
            "PyDrive==1.3.1\n",
            "pyemd==0.5.1\n",
            "pyglet==1.5.0\n",
            "Pygments==2.1.3\n",
            "pygobject==3.26.1\n",
            "pymc3==3.7\n",
            "PyMeeus==0.3.7\n",
            "pymongo==3.11.0\n",
            "pymystem3==0.2.0\n",
            "PyOpenGL==3.1.5\n",
            "pyparsing==2.4.7\n",
            "pyrsistent==0.16.0\n",
            "pysndfile==1.3.8\n",
            "PySocks==1.7.1\n",
            "pystan==2.19.1.1\n",
            "pytest==3.6.4\n",
            "python-apt==1.6.5+ubuntu0.3\n",
            "python-chess==0.23.11\n",
            "python-dateutil==2.8.1\n",
            "python-louvain==0.14\n",
            "python-slugify==4.0.1\n",
            "python-utils==2.4.0\n",
            "pytz==2018.9\n",
            "pyviz-comms==0.7.6\n",
            "PyWavelets==1.1.1\n",
            "PyYAML==3.13\n",
            "pyzmq==19.0.2\n",
            "qtconsole==4.7.5\n",
            "QtPy==1.9.0\n",
            "regex==2019.12.20\n",
            "requests==2.23.0\n",
            "requests-oauthlib==1.3.0\n",
            "resampy==0.2.2\n",
            "retrying==1.3.3\n",
            "rpy2==3.2.7\n",
            "rsa==4.6\n",
            "s3fs==0.4.2\n",
            "s3transfer==0.3.3\n",
            "sacremoses==0.0.43\n",
            "scikit-image==0.16.2\n",
            "scikit-learn==0.22.2.post1\n",
            "scipy==1.4.1\n",
            "screen-resolution-extra==0.0.0\n",
            "scs==2.1.2\n",
            "seaborn==0.10.1\n",
            "Send2Trash==1.5.0\n",
            "sentencepiece==0.1.91\n",
            "setuptools-git==1.2\n",
            "Shapely==1.7.0\n",
            "simplegeneric==0.8.1\n",
            "six==1.15.0\n",
            "sklearn==0.0\n",
            "sklearn-pandas==1.8.0\n",
            "smart-open==2.1.0\n",
            "snowballstemmer==2.0.0\n",
            "sortedcontainers==2.2.2\n",
            "spacy==2.2.4\n",
            "Sphinx==1.8.5\n",
            "sphinxcontrib-websupport==1.2.3\n",
            "SQLAlchemy==1.3.18\n",
            "sqlparse==0.3.1\n",
            "srsly==1.0.2\n",
            "statsmodels==0.10.2\n",
            "sympy==1.1.1\n",
            "tables==3.4.4\n",
            "tabulate==0.8.7\n",
            "tblib==1.7.0\n",
            "tensorboard==2.3.0\n",
            "tensorboard-plugin-wit==1.7.0\n",
            "tensorboardcolab==0.0.22\n",
            "tensorflow==2.3.0\n",
            "tensorflow-addons==0.8.3\n",
            "tensorflow-datasets==2.1.0\n",
            "tensorflow-estimator==2.3.0\n",
            "tensorflow-gcs-config==2.3.0\n",
            "tensorflow-hub==0.8.0\n",
            "tensorflow-metadata==0.22.2\n",
            "tensorflow-privacy==0.2.2\n",
            "tensorflow-probability==0.11.0\n",
            "termcolor==1.1.0\n",
            "terminado==0.8.3\n",
            "testpath==0.4.4\n",
            "text-unidecode==1.3\n",
            "textblob==0.15.3\n",
            "textgenrnn==1.4.1\n",
            "Theano==1.0.5\n",
            "thinc==7.4.0\n",
            "tifffile==2020.7.24\n",
            "tokenizers==0.8.1rc1\n",
            "toml==0.10.1\n",
            "toolz==0.10.0\n",
            "torch==1.6.0+cu101\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.3.1\n",
            "torchvision==0.7.0+cu101\n",
            "tornado==5.1.1\n",
            "tqdm==4.41.1\n",
            "traitlets==4.3.3\n",
            "transformers==3.0.2\n",
            "tweepy==3.6.0\n",
            "typeguard==2.7.1\n",
            "typing-extensions==3.7.4.2\n",
            "tzlocal==1.5.1\n",
            "umap-learn==0.4.6\n",
            "uritemplate==3.0.1\n",
            "urllib3==1.24.3\n",
            "vega-datasets==0.8.0\n",
            "wasabi==0.7.1\n",
            "wcwidth==0.2.5\n",
            "webencodings==0.5.1\n",
            "Werkzeug==1.0.1\n",
            "widgetsnbextension==3.5.1\n",
            "wordcloud==1.5.0\n",
            "wrapt==1.12.1\n",
            "xarray==0.15.1\n",
            "xgboost==0.90\n",
            "xkit==0.0.0\n",
            "xlrd==1.1.0\n",
            "xlwt==1.3.0\n",
            "yellowbrick==0.9.1\n",
            "zict==2.0.0\n",
            "zipp==3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GacFzw8HU_ZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa1-yKUxFSuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights('bert_weights.h5')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ8kSOEZIFnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('bert_weights.h5')"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LLMg2rOIlHX",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOZtajT0DUNy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "99a83039-0cf7-4570-c997-aa39d58b0c92"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(bert_weights.h5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-3e6f1c848afc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'bert_weights' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wytl3bj5rKXG",
        "colab_type": "text"
      },
      "source": [
        "# **DUAL BERT (TWO INPUT)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhBrORzXrKXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### CREATE SEQUENCES (id, mask, segments) FOR TRAIN AND TEST ###\n",
        "\n",
        "input_train = compute_input_arrays(X_train, ['headline','short_description'], tokenizer, MAX_SEQUENCE_LENGTH)\n",
        "input_test = compute_input_arrays(X_test, ['headline','short_description'], tokenizer, MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmsMDxjzrKXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dual_bert():\n",
        "    \n",
        "    opt = Adam(learning_rate=2e-5)\n",
        "    \n",
        "    id1 = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    id2 = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    \n",
        "    mask1 = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    mask2 = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    \n",
        "    atn1 = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    atn2 = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    \n",
        "    config = BertConfig() \n",
        "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
        "    bert_model1 = TFBertModel.from_pretrained('bert-base-uncased', config=config)\n",
        "    bert_model2 = TFBertModel.from_pretrained('bert-base-uncased', config=config)\n",
        "    \n",
        "    embedding1 = bert_model1(id1, attention_mask=mask1, token_type_ids=atn1)[0]\n",
        "    embedding2 = bert_model2(id2, attention_mask=mask2, token_type_ids=atn2)[0]\n",
        "    \n",
        "    x1 = GlobalAveragePooling1D()(embedding1)\n",
        "    x2 = GlobalAveragePooling1D()(embedding2)\n",
        "    \n",
        "    x = Concatenate()([x1, x2])\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    out = Dense(len(map_label), activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=[id1, mask1, atn1, id2, mask2, atn2], outputs=out)\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=opt)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLd9AH4NrKXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(33)\n",
        "os.environ['PYTHONHASHSEED'] = str(33)\n",
        "np.random.seed(33)\n",
        "random.seed(33)\n",
        "\n",
        "session_conf = tf.compat.v1.ConfigProto(\n",
        "    intra_op_parallelism_threads=1, \n",
        "    inter_op_parallelism_threads=1\n",
        ")\n",
        "sess = tf.compat.v1.Session(\n",
        "    graph=tf.compat.v1.get_default_graph(), \n",
        "    config=session_conf\n",
        ")\n",
        "tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "model = dual_bert()\n",
        "model.fit(input_train, y_train, epochs=3, batch_size=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmrxdzFKrKXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### PREDICT TEST ###\n",
        "\n",
        "pred_test = np.argmax(model.predict(input_test), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdrmaJK7rKXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classification_report([map_label[i] for i in y_test], [map_label[i] for i in pred_test]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp2i8pyZrKXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnf_matrix = confusion_matrix([map_label[i] for i in y_test], \n",
        "                              [map_label[i] for i in pred_test])\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "plot_confusion_matrix(cnf_matrix, classes=list(map_label.values()))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmd1Gt66rKXX",
        "colab_type": "text"
      },
      "source": [
        "# **SIAMESE BERT (TWO INPUT)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBVhWSGlrKXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def siamese_bert():\n",
        "    \n",
        "    opt = Adam(learning_rate=2e-5)\n",
        "    \n",
        "    id1 = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    id2 = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    \n",
        "    mask1 = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    mask2 = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    \n",
        "    atn1 = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    atn2 = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    \n",
        "    config = BertConfig()\n",
        "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
        "    bert_model = TFBertModel.from_pretrained('bert-base-uncased', config=config)\n",
        "    \n",
        "    embedding1 = bert_model(id1, attention_mask=mask1, token_type_ids=atn1)[0]\n",
        "    embedding2 = bert_model(id2, attention_mask=mask2, token_type_ids=atn2)[0]\n",
        "    \n",
        "    x1 = GlobalAveragePooling1D()(embedding1)\n",
        "    x2 = GlobalAveragePooling1D()(embedding2)\n",
        "    \n",
        "    x = Concatenate()([x1, x2])\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    out = Dense(len(map_label), activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=[id1, mask1, atn1, id2, mask2, atn2], outputs=out)\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=opt)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpZF4HDUrKXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(33)\n",
        "os.environ['PYTHONHASHSEED'] = str(33)\n",
        "np.random.seed(33)\n",
        "random.seed(33)\n",
        "\n",
        "session_conf = tf.compat.v1.ConfigProto(\n",
        "    intra_op_parallelism_threads=1, \n",
        "    inter_op_parallelism_threads=1\n",
        ")\n",
        "sess = tf.compat.v1.Session(\n",
        "    graph=tf.compat.v1.get_default_graph(), \n",
        "    config=session_conf\n",
        ")\n",
        "tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "model = siamese_bert()\n",
        "model.fit(input_train, y_train, epochs=3, batch_size=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UrI0vtCrKXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### PREDICT TEST ###\n",
        "\n",
        "pred_test = np.argmax(model.predict(input_test), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8aDM46GrKXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classification_report([map_label[i] for i in y_test], [map_label[i] for i in pred_test]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dpEo2VVrKXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnf_matrix = confusion_matrix([map_label[i] for i in y_test], \n",
        "                              [map_label[i] for i in pred_test])\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "plot_confusion_matrix(cnf_matrix, classes=list(map_label.values()))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}